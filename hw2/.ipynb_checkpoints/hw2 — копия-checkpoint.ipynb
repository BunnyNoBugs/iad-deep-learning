{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lg4NmvDKJNC7"
   },
   "source": [
    "# Домашнее задание 2. Классификация, детекция."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0t1P1QsJNC7"
   },
   "source": [
    "Оценка за часть 1 и часть 2 в этом дз -- по 5 баллов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvQJVe6oJNC7"
   },
   "source": [
    "## Часть 1. Классификация\n",
    "\n",
    "В этом задании потребуется обучить классификатор изображений. Будем работать с датасетом, название которого раскрывать не будем. Можете посмотреть самостоятельно на картинки, которые в датасете есть. В нём 200 классов и около 5 тысяч картинок на каждый класс. Классы пронумерованы, как нетрудно догадаться, от 0 до 199. Скачать датасет можно вот [тут](https://yadi.sk/d/BNR41Vu3y0c7qA).\n",
    "\n",
    "Структура датасета простая -- есть директории train и val, в которых лежат обучающие и валидационные данные. В train/ и val/ лежат директориии, соответствующие классам изображений, в которых лежат собственно сами изображения.\n",
    " \n",
    "__Задание__. Добейтесь accuracy **не менее 0.44**. Напишите краткий отчёт о проделанных экспериментах. Что сработало и что не сработало? Почему вы решили, сделать так, а не иначе? Обязательно указывайте ссылки на чужой код, если вы его используете. Обязательно ссылайтесь на статьи/блогпосты/вопросы на stackoverflow/видосы от (индийских) ютуберов/курсы/подсказки от Дяди Васи и прочие дополнительные материалы, если вы их используете. \n",
    "\n",
    "В коде ниже необходимо, чтобы код проходил все `assert`'ы.\n",
    "\n",
    "Необходимо написать функцию `predict` по шаблону ниже. Эта функция принимает на вход модель, даталоадер с валидационнами данными, criterion для подсчёта лосса и device, на котором будут производиться вычисления (определён ниже) и возвращает список лоссов по всем объектам, список из предсказанных классов для каждого объекта из даталоалера и список из настоящих классов для каждого объекта в даталоадере (и именно в таком порядке).\n",
    "\n",
    "__Использовать внешние данные для обучения строго запрещено__. Можно использовать предобученные модели из `torchvision`.\n",
    "\n",
    "__Критерии оценки__: Оценка вычисляется по простой формуле: min(5, 5 * Ваша accuracy / 0.44). Оценка округляется до десятых по арифметическим правилам.\n",
    "\n",
    "__Советы и указания__:\n",
    " - Наверняка вам потребуется много гуглить о классификации и о том, как заставить её работать. Это нормально, все гуглят. Но не забывайте, что нужно быть готовым за скатанный код отвечать на защите :)\n",
    " - Используйте аугментации. Для этого пользуйтесь модулем torchvision.transforms или библиотекой [albumentations](https://github.com/albumentations-team/albumentations)\n",
    " - (ещё раз) Можно файнтюнить предобученные модели из `torchvision`.\n",
    " - Рекомендуем написать вам сначала класс-датасет (или воспользоваться классом ImageFolder), который возвращает картинки и соответствующие им классы, а затем функции для трейна по шаблонам ниже. Однако делать это мы не заставляем. Если вам так неудобно, то можете писать код в удобном стиле. Однако учтите, что чрезмерное изменение нижеперечисленных шаблонов увеличит количество вопросов к вашему коду и повысит вероятность вызова на защиту :)\n",
    " - Валидируйте. Трекайте ошибки как можно раньше, чтобы не тратить время впустую.\n",
    " - Чтобы отладить код, пробуйте обучаться на маленькой части датасета. Когда вы поняли, что смогли всё отдебажить, переходите обучению по всему датасету\n",
    " - На каждый запуск делайте ровно одно изменение в модели/аугментации/оптимайзере, чтобы понять, что и как влияет на результат.\n",
    " - Фиксируйте random seed.\n",
    " - Начинайте с простых моделей и постепенно переходите к сложным. Обучение лёгких моделей экономит много времени.\n",
    " - Ставьте расписание на learning rate. Уменьшайте его, когда лосс на валидации перестаёт убывать.\n",
    " - Советуем использовать гпу. Если у вас его нет, используйте google colab. Если вам неудобно его использовать на постоянной основе, напишите и отладьте весь код локально на CPU, а затем запустите уже написанный ноутбук в колабе. Авторское решение задания достигает требуемой точности в колабе за 15 минут обучения.\n",
    " \n",
    "Good luck & have fun! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN3u5O4kJNC7"
   },
   "source": [
    "Материалы, которые я использовал:\n",
    "\n",
    "* https://albumentations.ai/docs/examples/pytorch_classification/\n",
    "* https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DXv1bTDrJNC7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torchvision.models import resnet18, resnet152\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "bVkn0yUSJNC7",
    "outputId": "cc347f65-7c5e-4762-99e9-ce3369fdb944"
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomAffine(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "        transforms.RandomGrayscale(),\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = ImageFolder('./dataset/train', transform=train_transform)\n",
    "val_dataset = ImageFolder('./dataset/val', transform=val_transform)\n",
    "\n",
    "# train_dataset = MyDataset(\"./dataset/dataset/train\", transform=train_transform)\n",
    "# val_dataset = MyDataset(\"./dataset/dataset/val\", transform=val_transform)\n",
    "# REPLACE ./dataset/dataset WITH THE FOLDER WHERE YOU DOWNLOADED AND UNZIPPED THE DATASET\n",
    "# OR USE torchvision.datasets.ImageFolder INSTEAD OF MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3Dvk-bOaJNC7",
    "outputId": "0cb7b002-e7db-4195-b2a7-07918481cbd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests passed\n"
     ]
    }
   ],
   "source": [
    "# Just very simple checks\n",
    "assert isinstance(train_dataset[0], tuple)\n",
    "assert len(train_dataset[0]) == 2\n",
    "assert isinstance(train_dataset[1][1], int)\n",
    "print(\"tests passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3IY0TOtLJNC8"
   },
   "outputs": [],
   "source": [
    "def plot_history(train_history, val_history, title='loss'):\n",
    "    plt.figure()\n",
    "    plt.title('{}'.format(title))\n",
    "    plt.plot(train_history, label='train', zorder=1)\n",
    "    \n",
    "    points = np.array(val_history)\n",
    "    steps = list(range(0, len(train_history) + 1, int(len(train_history) / len(val_history))))[1:]\n",
    "    \n",
    "    plt.scatter(steps, val_history, marker='+', s=180, c='orange', label='val', zorder=2)\n",
    "    plt.xlabel('train steps')\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-tkYTl1_JNC8"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataloader, criterion, optimizer, device=\"cuda:0\", return_log=False):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    all_losses = []\n",
    "    acc_log = []\n",
    "    total_predictions = np.array([])#.reshape((0, ))\n",
    "    total_labels = np.array([])#.reshape((0, ))\n",
    "    with tqdm(total=len(train_dataloader), file=sys.stdout) as prbar:\n",
    "        for images, labels in train_dataloader:\n",
    "            # Move Batch to GPU\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predicted = model(images)\n",
    "            loss = criterion(predicted, labels)\n",
    "            # Update weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # Update description for tqdm\n",
    "            accuracy = (predicted.argmax(1) == labels).float().mean()\n",
    "            prbar.set_description(\n",
    "                f\"Loss: {round(loss.item(), 4)} \"\n",
    "                f\"Accuracy: {round(accuracy.item() * 100, 4)}\"\n",
    "            )\n",
    "            prbar.update(1)\n",
    "            total_loss += loss.item()\n",
    "            total_predictions = np.append(total_predictions, predicted.argmax(1).cpu().detach().numpy())\n",
    "            total_labels = np.append(total_labels, labels.cpu().detach().numpy())\n",
    "            num_batches += 1\n",
    "            all_losses.append(loss.detach().item())\n",
    "            acc_log.append(accuracy.detach().item())\n",
    "    metrics = {'loss': total_loss / num_batches}\n",
    "    metrics.update({\"accuracy\": (total_predictions == total_labels).mean()})\n",
    "    if return_log:\n",
    "        return metrics, all_losses, acc_log\n",
    "    else:\n",
    "        return metrics\n",
    "\n",
    "\n",
    "def predict(model, val_dataloader, criterion, device=\"cuda:0\"):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    total_predictions = np.array([])\n",
    "    total_labels = np.array([])\n",
    "    with tqdm(total=len(val_dataloader), file=sys.stdout) as prbar:\n",
    "        for images, labels in val_dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predicted = model(images)\n",
    "            loss = criterion(predicted, labels)\n",
    "            accuracy = (predicted.argmax(1) == labels).float().mean()\n",
    "            prbar.set_description(\n",
    "                f\"Loss: {round(loss.item(), 4)} \"\n",
    "                f\"Accuracy: {round(accuracy.item() * 100, 4)}\"\n",
    "            )\n",
    "            prbar.update(1)\n",
    "            total_loss += loss.item()\n",
    "            total_predictions = np.append(total_predictions, predicted.argmax(1).cpu().detach().numpy())\n",
    "            total_labels = np.append(total_labels, labels.cpu().detach().numpy())\n",
    "            num_batches += 1\n",
    "    metrics = {'loss': total_loss / num_batches}\n",
    "    metrics.update({'accuracy': (total_predictions == total_labels).mean()})\n",
    "    # predict val_dataloader and print and return the validation accuracy\n",
    "    predicted_classes = total_predictions\n",
    "    true_classes = total_labels\n",
    "    return metrics, predicted_classes, true_classes\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, criterion, optimizer, device=\"cuda:0\", n_epochs=10, scheduler=None):\n",
    "    all_train_losses = []\n",
    "    epoch_train_losses = []\n",
    "    epoch_eval_losses = []\n",
    "    train_acc_log = []\n",
    "    eval_acc_log = []\n",
    "    for epoch in n_epochs:\n",
    "        # Train step\n",
    "        print(f\"Train Epoch: {epoch}\")\n",
    "        train_metrics, one_epoch_train_losses, one_epoch_acc_log = train_one_epoch(\n",
    "            model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            return_log=True,\n",
    "            criterion=criterion\n",
    "        )\n",
    "        # Save train losses\n",
    "        all_train_losses.extend(one_epoch_train_losses)\n",
    "        epoch_train_losses.append(train_metrics['loss'])\n",
    "        # Save train accuracy log\n",
    "        train_acc_log.extend(one_epoch_acc_log)\n",
    "        # Eval step\n",
    "        print(f\"Validation Epoch: {epoch}\")\n",
    "        with torch.no_grad():\n",
    "            validation_metrics = predict(\n",
    "                model=model,\n",
    "                val_dataloader=val_dataloader,\n",
    "                criterion=criterion\n",
    "            )[0]\n",
    "        # Save eval losses\n",
    "        epoch_eval_losses.append(validation_metrics['loss'])\n",
    "        # Save eval accuracy\n",
    "        eval_acc_log.append(validation_metrics['accuracy'])\n",
    "        # Take scheduler step\n",
    "        scheduler.step()\n",
    "        # Plot loss and accuracy\n",
    "        clear_output()\n",
    "        plot_history(all_train_losses, epoch_eval_losses, 'loss')\n",
    "        plot_history(train_acc_log, eval_acc_log, 'accuracy')\n",
    "        # Train, evaluate, print accuracy, make a step of scheduler or whatever you want..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3Y_FDL9dJNC8",
    "outputId": "8eb88865-1b81-41f4-c738-71bbd59ffc8f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (23): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (24): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (25): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (26): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (27): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (28): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (29): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (30): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (31): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (32): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (33): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (34): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (35): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = resnet152(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = nn.Linear(2048, 200)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pXenztaFJNC8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.fc.parameters(), 1e-4)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lambda1 = lambda epoch: 0.95 ** epoch\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "n_epochs = range(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcx6L6t0JNC8"
   },
   "source": [
    "Простой тест на проверку правильности написанного кода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "R4dr9LL9JNC8",
    "outputId": "b6e20fc5-2c50-400d-80a8-759282f2f701",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.3949 Accuracy: 0.0:  28%|██████████████▊                                       | 11/40 [00:24<01:04,  2.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7ba8bf5b2b50>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall_losses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tests passed\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-184b0239944d>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(model, val_dataloader, criterion, device)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             prbar.set_description(\n\u001b[1;32m---> 55\u001b[1;33m                 \u001b[1;34mf\"Loss: {round(loss.item(), 4)} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m                 \u001b[1;34mf\"Accuracy: {round(accuracy.item() * 100, 4)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_losses, predicted_labels, true_labels = predict(model, val_dataloader, criterion, device)\n",
    "assert len(predicted_labels) == len(val_dataset)\n",
    "accuracy = accuracy_score(predicted_labels, true_labels)\n",
    "print(\"tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_BQ787-JNC8"
   },
   "source": [
    "Запустить обучение можно в ячейке ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1Ha_zccUJNC8",
    "outputId": "7aad9187-3ccc-4f21-c03f-795df607ba3a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0\n",
      "Loss: 5.3703 Accuracy: 0.7812:   1%|▎                                                  | 2/391 [00:08<28:05,  4.33s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-73764bd69175>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-184b0239944d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, val_dataloader, criterion, optimizer, device, n_epochs, scheduler)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;31m# Train step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Train Epoch: {epoch}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         train_metrics, one_epoch_train_losses, one_epoch_acc_log = train_one_epoch(\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-184b0239944d>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, train_dataloader, criterion, optimizer, device, return_log)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtotal_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#.reshape((0, ))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mprbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[1;31m# Move Batch to GPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m    150\u001b[0m         \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    891\u001b[0m         \"\"\"\n\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"P\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\PIL\\ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m                             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m                             \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, val_dataloader, criterion, optimizer, device, n_epochs, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iyr3rBLGJNC8"
   },
   "source": [
    "После всех экспериментов, которые вы проделали, выберите лучшую из своих моделей, запустите функцию evaluate. Эта функция должна брать на вход модель и даталоадер с валидационными данными и возврашать accuracy, посчитанную на этом датасете."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GIWiMfgJNC8"
   },
   "source": [
    "После 10 эпох получили accuracy 0.43. Попробуем дообучить еще 1-2 эпохи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfE5qHlGJNC8",
    "outputId": "dd96ef52-431d-4503-a93b-3a277fbcf077"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epochs_2 = range(10, 12)\n",
    "\n",
    "train(model, train_dataloader, val_dataloader, criterion, optimizer, device, n_epochs_2, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2ZgCfoxJNC8",
    "outputId": "9cd8df70-7640-496c-b563-a6a506c91704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.8588 Accuracy: 31.25: 100%|████████████████████████████████████████████████████| 40/40 [00:51<00:00,  1.28s/it]\n",
      "Accuracy составила 0.4433\n",
      "Оценка за это задание составит 5 баллов\n"
     ]
    }
   ],
   "source": [
    "all_losses, predicted_labels, true_labels = predict(model, val_dataloader, criterion, device)\n",
    "assert len(predicted_labels) == len(val_dataset)\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f'Accuracy составила {accuracy}')\n",
    "print(\"Оценка за это задание составит {} баллов\".format(min(5, 5*accuracy / 0.44)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yuBfW7WJNC8"
   },
   "source": [
    "__Ваш отчёт о проделанных экспериментах__: я начал с предобученной ResNet18 и обучал последний полносвязный слой. После 8 эпох она выдавала качество accuracy около 0.25 и больше не обучалась (однако надо учесть, что при обучении ResNet я забыл нормализовать картинки). Тогда я решил увеличить количество слоев и взял ResNet152. После 10 эпох она выдала accuracy 0.43, а после 12 -- 0.43. Во всех случаях я применял к картинкам из train различные аугментации. Я использовал расписание на скорость обучения: learning rate составлял 0.95 в степени номера эпохи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIYeYXNHJNC8"
   },
   "outputs": [],
   "source": [
    "with open('task_1_model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrV-J6JjJNC9"
   },
   "source": [
    "## Часть 2. Object detection.\n",
    "\n",
    "В этом задании потребуется обучить детектор фруктов на изображении. Датасет можно скачать [отсюда](https://yadi.sk/d/UPwQB7OZrB48qQ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6tKMo--JNC9"
   },
   "source": [
    "Материалы, которые я использовал:\n",
    "\n",
    "* https://albumentations.ai/docs/examples/example_bboxes/\n",
    "* https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gjBoi45JNC9",
    "outputId": "51912bb1-658a-4175-dd86-a67a704aaa08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xmltodict\n",
      "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n",
      "Installing collected packages: xmltodict\n",
      "Successfully installed xmltodict-0.12.0\n"
     ]
    }
   ],
   "source": [
    "# we will need this library to process the labeling\n",
    "! pip install xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ETRE7LnyJNC9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import xmltodict\n",
    "import json\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "import xmltodict, json\n",
    "\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from torch import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxpdF3ItJNC9",
    "outputId": "bff7b0a2-0548-43bb-b52e-4776c7a3dc07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4zRVjubDJkZP",
    "outputId": "c27c670b-b214-4712-d8c3-7edd7a7fe66b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2cij-tC4JgRp"
   },
   "outputs": [],
   "source": [
    "! unzip -q '/content/drive/MyDrive/Colab Notebooks/ИАД — Современные методы машинного обучения/hw2/archive.zip' -d 'archive/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdrsAEB0OLct",
    "outputId": "ac9cbf7b-0252-423e-c646-0a8ee7986304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive  drive\tsample_data\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnbmfidcJNC9"
   },
   "source": [
    "Датасет мы за вас написали."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zKNcEJz0JNC9"
   },
   "outputs": [],
   "source": [
    "class2tag = {\"apple\": 1, \"orange\": 2, \"banana\": 3}\n",
    "\n",
    "\n",
    "class FruitDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.images = []\n",
    "        self.annotations = []\n",
    "        self.transform = transform\n",
    "        for annotation in glob.glob(data_dir + \"/*xml\"):\n",
    "            image_fname = os.path.splitext(annotation)[0] + \".jpg\"\n",
    "            self.images.append(cv2.cvtColor(cv2.imread(image_fname), cv2.COLOR_BGR2RGB))\n",
    "            with open(annotation) as f:\n",
    "                annotation_dict = xmltodict.parse(f.read())\n",
    "            bboxes = []\n",
    "            labels = []\n",
    "            objects = annotation_dict[\"annotation\"][\"object\"]\n",
    "            if not isinstance(objects, list):\n",
    "                objects = [objects]\n",
    "            for obj in objects:\n",
    "                bndbox = obj[\"bndbox\"]\n",
    "                bbox = [bndbox[\"xmin\"], bndbox[\"ymin\"], bndbox[\"xmax\"], bndbox[\"ymax\"]]\n",
    "                bbox = list(map(int, bbox))\n",
    "                bboxes.append(torch.tensor(bbox))\n",
    "                labels.append(class2tag[obj[\"name\"]])\n",
    "            self.annotations.append(\n",
    "                {\"boxes\": torch.stack(bboxes).float(), \"labels\": torch.tensor(labels)}\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.transform:\n",
    "            # the following code is correct if you use albumentations\n",
    "            # if you use torchvision transforms you have to modify it =)\n",
    "            res = {\n",
    "                'image': self.transform(self.images[i]),\n",
    "                'bboxes': self.annotations[i][\"boxes\"],\n",
    "                'labels': self.annotations[i][\"labels\"],\n",
    "            }\n",
    "            return res[\"image\"], {\n",
    "                \"boxes\": torch.tensor(res[\"bboxes\"]),\n",
    "                \"labels\": torch.tensor(res[\"labels\"]),\n",
    "            }\n",
    "        else:\n",
    "            return self.images[i], self.annotations[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKUlgdQMJNC9"
   },
   "source": [
    "Выпишем кое-какую техническую работу, которая уже была на семинаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NlrZOS0WJNC9"
   },
   "outputs": [],
   "source": [
    "def intersection_over_union(dt_bbox, gt_bbox):\n",
    "    \"\"\"\n",
    "    Intersection over Union between two bboxes\n",
    "    :param dt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
    "    :param gt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
    "    :return : intersection over union\n",
    "    \"\"\"\n",
    "\n",
    "    ## TODO YOUR CODE\n",
    "\n",
    "    intersection_bbox = np.array(\n",
    "        [\n",
    "            max(dt_bbox[0], gt_bbox[0]),\n",
    "            max(dt_bbox[1], gt_bbox[1]),\n",
    "            min(dt_bbox[2], gt_bbox[2]),\n",
    "            min(dt_bbox[3], gt_bbox[3]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    intersection_area = max(intersection_bbox[2] - intersection_bbox[0], 0) * max(\n",
    "        intersection_bbox[3] - intersection_bbox[1], 0\n",
    "    )\n",
    "    area_dt = (dt_bbox[2] - dt_bbox[0]) * (dt_bbox[3] - dt_bbox[1])\n",
    "    area_gt = (gt_bbox[2] - gt_bbox[0]) * (gt_bbox[3] - gt_bbox[1])\n",
    "\n",
    "    union_area = area_dt + area_gt - intersection_area\n",
    "\n",
    "    iou = intersection_area / union_area\n",
    "    return iou\n",
    "\n",
    "def evaluate_sample(target_pred, target_true, iou_threshold=0.5):\n",
    "    gt_bboxes = target_true[\"boxes\"].numpy()\n",
    "    gt_labels = target_true[\"labels\"].numpy()\n",
    "\n",
    "    dt_bboxes = target_pred[\"boxes\"].numpy()\n",
    "    dt_labels = target_pred[\"labels\"].numpy()\n",
    "    dt_scores = target_pred[\"scores\"].numpy()\n",
    "    \n",
    "    results = []\n",
    "    for detection_id in range(len(dt_labels)):\n",
    "        dt_bbox = dt_bboxes[detection_id, :]\n",
    "        dt_label = dt_labels[detection_id]\n",
    "        dt_score = dt_scores[detection_id]\n",
    "\n",
    "        detection_result_dict = {\"score\": dt_score}\n",
    "\n",
    "        max_IoU = 0\n",
    "        max_gt_id = -1\n",
    "        for gt_id in range(len(gt_labels)):\n",
    "            gt_bbox = gt_bboxes[gt_id, :]\n",
    "            gt_label = gt_labels[gt_id]\n",
    "\n",
    "            if gt_label != dt_label:\n",
    "                continue\n",
    "\n",
    "            if intersection_over_union(dt_bbox, gt_bbox) > max_IoU:\n",
    "                max_IoU = intersection_over_union(dt_bbox, gt_bbox)\n",
    "                max_gt_id = gt_id\n",
    "\n",
    "        if max_gt_id >= 0 and max_IoU >= iou_threshold:\n",
    "            detection_result_dict[\"TP\"] = 1\n",
    "            gt_labels = np.delete(gt_labels, max_gt_id, axis=0)\n",
    "            gt_bboxes = np.delete(gt_bboxes, max_gt_id, axis=0)\n",
    "\n",
    "        else:\n",
    "            detection_result_dict[\"TP\"] = 0\n",
    "\n",
    "        results.append(detection_result_dict)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    results = []\n",
    "    model.eval()\n",
    "    nbr_boxes = 0\n",
    "    print(test_loader)\n",
    "    with torch.no_grad():\n",
    "        for batch, (images, targets_true) in enumerate(test_loader):\n",
    "            images = list(image.to(device).float() for image in images)\n",
    "            targets_pred = model(images)\n",
    "            targets_true = [\n",
    "                {k: v.cpu().float() for k, v in t.items()} for t in targets_true\n",
    "            ]\n",
    "            targets_pred = [\n",
    "                {k: v.cpu().float() for k, v in t.items()} for t in targets_pred\n",
    "            ]\n",
    "\n",
    "            for i in range(len(targets_true)):\n",
    "                target_true = targets_true[i]\n",
    "                target_pred = targets_pred[i]\n",
    "                nbr_boxes += target_true[\"labels\"].shape[0]\n",
    "\n",
    "                results.extend(evaluate_sample(target_pred, target_true))\n",
    "\n",
    "    results = sorted(results, key=lambda k: k[\"score\"], reverse=True)\n",
    "\n",
    "    acc_TP = np.zeros(len(results))\n",
    "    acc_FP = np.zeros(len(results))\n",
    "    recall = np.zeros(len(results))\n",
    "    precision = np.zeros(len(results))\n",
    "\n",
    "    if results[0][\"TP\"] == 1:\n",
    "        acc_TP[0] = 1\n",
    "    else:\n",
    "        acc_FP[0] = 1\n",
    "\n",
    "    for i in range(1, len(results)):\n",
    "        acc_TP[i] = results[i][\"TP\"] + acc_TP[i - 1]\n",
    "        acc_FP[i] = (1 - results[i][\"TP\"]) + acc_FP[i - 1]\n",
    "\n",
    "        precision[i] = acc_TP[i] / (acc_TP[i] + acc_FP[i])\n",
    "        recall[i] = acc_TP[i] / nbr_boxes\n",
    "\n",
    "    return auc(recall, precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRmLHRdSJNC9"
   },
   "source": [
    "Вам мы оставляем творческую часть =)\n",
    "\n",
    "__Задание__. Обучите модель для object detection на __обучающем__ датасете и добейтесь PR-AUC не менее __0.91__ на  __тестовом__.\n",
    "\n",
    " - Создайте модель и оптимайзер\n",
    " - Напишите функцию обучения модели\n",
    " - Используйте аугментации\n",
    " \n",
    "Использовать аугментации для обучения __обязательно__. Они дадут 1 балл из 5. Пользуйтесь модулем torchvision.transforms или библиотекой albumentations (о которой говорилось ранее). Последняя библиотека особенно удобна, поскольку умеет сама вычислять новые координаты bounding box'ов после трансформаций картинки. Советуем обратить внимание на следующий [гайд](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/). Обратите внимание, что код, написанный в датасете выше, верен только если вы используете albumentations. Если вы выбрали путь torchvision.transforms, вам потребуется метод `__getitem__` изменить (что-то типа `return self.transform(self.images[i])`; однако в таком случае вычислять новые координаты bounding box'ов после трансформаций вам придётся вручную =))\n",
    "\n",
    "Оставшиеся 4 балла вычисляются по простой формуле: __min(4, 4 * Ваш auc / 0.91)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VgGMu4nEyQdt"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataloader, optimizer, device):\n",
    "    n = 0\n",
    "    global_loss = 0\n",
    "    print(train_dataloader)\n",
    "    for images, targets in train_dataloader:\n",
    "        images = list(image.to(device).float() for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        # print('TARGETS TO DICT')\n",
    "        # print(targets)\n",
    "        \n",
    "        for i in images:\n",
    "            if not torch.isfinite(i).all():\n",
    "                print(i)\n",
    "        dict_loss = model(images, targets)\n",
    "        # print('PREDICTIONS')\n",
    "        # print(dict_loss)\n",
    "        losses = sum(loss for loss in dict_loss.values())\n",
    "\n",
    "        with autograd.detect_anomaly():\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        n += 1\n",
    "        global_loss += float(losses.cpu().detach().numpy())\n",
    "\n",
    "        if n % 10 == 0:\n",
    "            print(\"Loss value after {} batches is {}\".format(n, round(global_loss / n, 2)))\n",
    "\n",
    "    return global_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tgV8Zjl3JNC9"
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, optimizer, device, n_epochs=10):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.eval()\n",
    "        a = evaluate(model, val_dataloader, device=device)\n",
    "        # print(\"AUC ON TEST: {.4f}\".format(a))\n",
    "        print(\"AUC ON TEST: {}\".format(a))\n",
    "        model.train()\n",
    "        train_one_epoch(model, train_dataloader, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ujZT6HZnJNC9"
   },
   "outputs": [],
   "source": [
    "train_transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.RandomHorizontalFlip(0.5),\n",
    "\n",
    "    ],\n",
    ")\n",
    "val_transform = T.Compose([\n",
    "        T.ToTensor()\n",
    "    ],\n",
    ")\n",
    "# HINT: TRAIN TRANSFORM OBVIOUSLY SHOULD BE HARDER THAN THOSE FOR VALIDATION\n",
    "\n",
    "train_dataset = FruitDataset(\"archive/train_zip/train\", transform=train_transform)\n",
    "val_dataset = FruitDataset(\"archive/test_zip/test\", transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GUhmSWoxL7Po"
   },
   "outputs": [],
   "source": [
    "# del model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "L2zSpjcrJNC9"
   },
   "outputs": [],
   "source": [
    "model = fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "num_classes = 4\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "# HINT: USE MATERIALS FROM THE SEMINAR\n",
    "# YOU CAN USE torchvision.models AND torchvision.models.detection\n",
    "# READ OFFICIAL DOCS FOR MORE INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8F1kn6JAJNC9"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "x9myaIIiJNC9"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=2, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=1, shuffle=False, num_workers=4, collate_fn=collate_fn)\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a-iwJg6hJNC9",
    "outputId": "524c0d34-64bc-4207-a7b1-49e5326d8d46"
   },
   "outputs": [],
   "source": [
    "# For Training\n",
    "images, targets = next(iter(train_dataloader))\n",
    "images = list(image.to(device).float() for image in images)\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "output = model(images, targets)   # Returns losses and detections\n",
    "print('output')\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400).to(device), torch.rand(3, 500, 400).to(device)]\n",
    "predictions = model(x)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 777
    },
    "id": "bm9oHkeiJNC9",
    "outputId": "e3502346-12ae-445b-b2f0-09343ac90130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f51b71f8f98>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC ON TEST: 0.740471206988538\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f51b71f8f28>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss value after 10 batches is 0.25\n",
      "Loss value after 20 batches is 0.27\n",
      "Loss value after 30 batches is 0.29\n",
      "Loss value after 40 batches is 0.25\n",
      "Loss value after 50 batches is 0.25\n",
      "Loss value after 60 batches is 0.25\n",
      "Loss value after 70 batches is 0.26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-6a3941437448>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-878879100e12>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, device, n_epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC ON TEST: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-a3a366abb1d4>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, val_dataloader, optimizer, device, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBFLhjAuJNC9"
   },
   "source": [
    "__Выведите итоговое качество модели__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qaSTdOigJNC9",
    "outputId": "85893490-f7f1-4e05-dfad-f293eb407f60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f51b71f8f98>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка за это задание составит 3.6475853696324316 баллов\n"
     ]
    }
   ],
   "source": [
    "your_auc = evaluate(model, val_dataloader, device)\n",
    "print(\"Оценка за это задание составит {} баллов\".format(min(4, 4 * your_auc / 0.91)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FydJ5OuMJNC9"
   },
   "source": [
    "Нарисуйте предсказанные bounding box'ы для любых двух картинок из __тестового__ датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "coOcholdJNC9",
    "outputId": "73e26040-2912-408c-d732-828dedac883d"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-eda2b4080fc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"In training mode, targets should be passed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: In training mode, targets should be passed"
     ]
    }
   ],
   "source": [
    "image, labels = next(iter(train_dataset))\n",
    "pred = model(image.unsqueeze(0).to(device))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7u0qXJQK1_I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "4waOF6GbJNC9",
    "outputId": "d2bb5b00-daef-4f2c-ca62-bf219db6255c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-13ee8147bb4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'boxes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "image = torchvision.transforms.ToPILImage()(image)\n",
    "draw = ImageDraw.Draw(image)\n",
    "for box in labels['boxes']:\n",
    "    draw.rectangle([(box[0], box[1]), (box[2], box[3])])\n",
    "    \n",
    "for box in pred['boxes']:\n",
    "    draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline='red')\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0A2Ldb89JNC9"
   },
   "source": [
    "## Бонус (10 баллов).\n",
    "\n",
    "__Задание__. В части с классификацией добейтесь выполнения хотя бы одного из перечисленных критериев:\n",
    " - Достичь accuracy не менее 0.56, **не используя resize картинок**, а также **не используя предобученные сети**;\n",
    " - Достичь accuracy не менее 0.86, но использовать resize и предобученные из torchvision сети разрешается. \n",
    " \n",
    "Напишите отчёт о проделанных экспериментах.\n",
    "\n",
    "__Критерии оценки__. Оценка за бонусную часть равна 10, если вы выполнили хоть один из вышеперечисленных критериев и 0 в противном случае.\n",
    "\n",
    "__Иных оценок кроме 0 и 10 не предусмотрено__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "htFXBfr_JNC9"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiSG7EWwJNC9"
   },
   "source": [
    "## Бонус (0 баллов).\n",
    "\n",
    "__Задание 1__. Скиньте ниже смешную картинку, желательно про машинное обучение. На картинке не должно быть никаких упоминаний лектора, семинаристов и ассистентов этого курса.\n",
    "\n",
    "__Задание 2__. Расскажите, как вам задание? Что понравилось, что не понравилось, что можно улучшить? Мы примем во внимание любой фидбек."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw2 task 2 torchvisions transforms.ipynb\"\"",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
